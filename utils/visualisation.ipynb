{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.nn import functional as F\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "class CombinedTransformerVisualizer:\n",
    "    def __init__(self, model, processor, device='cuda'):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Target the output of the last transformer block\n",
    "        self.target_layer = self.model.vision_model.encoder.layers[-1]\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output[0]\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        # Register hooks for GradCAM\n",
    "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.backward_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "        \n",
    "        # Register hooks for attention maps on all encoder layers\n",
    "        self.attention_hooks = []\n",
    "        for layer in self.model.vision_model.encoder.layers:\n",
    "            hook = layer.self_attn.register_forward_hook(\n",
    "                lambda module, input, output: self._attention_hook(module, output)\n",
    "            )\n",
    "            self.attention_hooks.append(hook)\n",
    "\n",
    "    def _attention_hook(self, module, output):\n",
    "        \"\"\"Hook to capture attention maps during forward pass.\"\"\"\n",
    "        if isinstance(output, tuple) and len(output) > 1:\n",
    "            attention_weights = output[1]  # Usually, the attention weights are in the second element\n",
    "            \n",
    "            if attention_weights is not None:\n",
    "                print(\"Captured attention weights:\", attention_weights.shape)  # Debug print for attention weights\n",
    "                self.attention_maps.append(attention_weights.detach())  # Save the attention weights\n",
    "            else:\n",
    "                print(\"Warning: Attention weights are None.\")  # If no weights, print a warning\n",
    "        else:\n",
    "            print(\"Unexpected output format from attention layer:\", output)\n",
    "\n",
    "    def _perform_attention_rollout(self):\n",
    "        \"\"\"Perform attention rollout across all layers.\"\"\"\n",
    "        if not self.attention_maps:\n",
    "            raise ValueError(\"No attention maps captured. Check if the forward pass was successful.\")\n",
    "            \n",
    "        # Average attention heads per layer\n",
    "        averaged_attentions = [attn.mean(dim=1) for attn in self.attention_maps]\n",
    "        \n",
    "        # Start with identity matrix\n",
    "        batch_size, seq_len, _ = averaged_attentions[0].shape\n",
    "        accumulated = torch.eye(seq_len).unsqueeze(0).to(self.device)\n",
    "        accumulated = accumulated.repeat(batch_size, 1, 1)\n",
    "        \n",
    "        # Accumulate attention through layers\n",
    "        for attn in averaged_attentions:\n",
    "            accumulated = torch.bmm(attn, accumulated)\n",
    "        \n",
    "        # Get attention for tokens (exclude CLS token)\n",
    "        rollout = accumulated[:, 0, 1:]\n",
    "        \n",
    "        return rollout\n",
    "\n",
    "    def _process_attention_map(self, attention_map, image_size):\n",
    "        \"\"\"Process attention map for visualization.\"\"\"\n",
    "        attn = attention_map.cpu().numpy()\n",
    "        grid_size = int(np.sqrt(attn.shape[-1]))\n",
    "        attn = attn.reshape(grid_size, grid_size)\n",
    "        attn = cv2.resize(attn, (image_size[0], image_size[1]))\n",
    "        attn = gaussian_filter(attn, sigma=2)\n",
    "        attn = (attn - attn.min()) / (attn.max() - attn.min() + 1e-8)\n",
    "        return attn\n",
    "\n",
    "    def apply_threshold(self, cam, threshold=0.2):\n",
    "        \"\"\"Apply threshold to focus on high attention regions.\"\"\"\n",
    "        cam[cam < threshold] = 0\n",
    "        return cam\n",
    "\n",
    "    def generate_visualizations(self, image_path, save_path=None):\n",
    "        \"\"\"Generate both GradCAM and attention visualizations.\"\"\"\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        original_size = image.size\n",
    "        image_resized = image.resize((384, 384), Image.Resampling.LANCZOS)\n",
    "        inputs = self.processor(images=image_resized, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Clear previous attention maps\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Forward pass for attention maps\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.vision_model(\n",
    "                inputs['pixel_values'],\n",
    "                output_attentions=True,  # Ensure this is set to True\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        # Get attention rollout\n",
    "        rollout = self._perform_attention_rollout()\n",
    "        attention_map = self._process_attention_map(rollout[0], original_size)\n",
    "        \n",
    "        # Clear gradients and perform forward/backward pass for GradCAM\n",
    "        self.model.zero_grad()\n",
    "        outputs = self.model.vision_model(**inputs)\n",
    "        target = outputs.last_hidden_state.mean(dim=1).sum()\n",
    "        target.backward()\n",
    "        \n",
    "        if self.gradients is None or self.activations is None:\n",
    "            print(\"Error: Gradients or activations are None.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate GradCAM\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=1)\n",
    "        cam = torch.zeros(self.activations.shape[1], dtype=self.activations.dtype).to(self.device)\n",
    "        \n",
    "        for i in range(1, self.activations.shape[1]):\n",
    "            cam[i] = torch.sum(pooled_gradients[0] * self.activations[0, i])\n",
    "        \n",
    "        # Process GradCAM\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.detach().cpu().numpy()\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
    "        cam = self.apply_threshold(cam)\n",
    "        \n",
    "        # Reshape and resize GradCAM\n",
    "        grid_size = int(np.sqrt(len(cam) - 1))\n",
    "        cam_reshaped = cam[1:].reshape(grid_size, grid_size)\n",
    "        cam_resized = cv2.resize(cam_reshaped, original_size)\n",
    "        \n",
    "        # Create attention overlay\n",
    "        image_array = np.array(image)\n",
    "        attention_heatmap = cv2.applyColorMap(np.uint8(attention_map * 255), cv2.COLORMAP_JET)\n",
    "        attention_overlay = cv2.addWeighted(image_array, 0.7, attention_heatmap, 0.3, 0)\n",
    "        \n",
    "        # Create GradCAM overlay\n",
    "        gradcam_heatmap = cv2.applyColorMap(np.uint8(cam_resized * 255), cv2.COLORMAP_JET)\n",
    "        gradcam_overlay = cv2.addWeighted(image_array, 0.7, gradcam_heatmap, 0.3, 0)\n",
    "        \n",
    "        # Create GradCAM on Attention Overlay\n",
    "        gradcam_on_attention = cv2.addWeighted(attention_overlay, 0.5, gradcam_overlay, 0.5, 0)\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(25, 5))\n",
    "        \n",
    "        plt.subplot(1, 6, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 2)\n",
    "        plt.imshow(cam_resized, cmap='jet')\n",
    "        plt.title('GradCAM Heatmap')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 3)\n",
    "        plt.imshow(attention_map, cmap='jet')\n",
    "        plt.title('Attention Map')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 4)\n",
    "        plt.imshow(cv2.cvtColor(gradcam_overlay, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('GradCAM Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 5)\n",
    "        plt.imshow(cv2.cvtColor(attention_overlay, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Attention Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 6)\n",
    "        plt.imshow(cv2.cvtColor(gradcam_on_attention, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('GradCAM on Attention Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        \n",
    "        return {\n",
    "            'gradcam': cam_resized,\n",
    "            'attention_map': attention_map,\n",
    "            'gradcam_overlay': gradcam_overlay,\n",
    "            'attention_overlay': attention_overlay,\n",
    "            'gradcam_on_attention': gradcam_on_attention\n",
    "        }\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up hooks.\"\"\"\n",
    "        self.forward_handle.remove()\n",
    "        self.backward_handle.remove()\n",
    "        for hook in self.attention_hooks:\n",
    "            hook.remove()\n",
    "\n",
    "\n",
    "def analyze_image(model_path, image_path, device='cuda'):\n",
    "    try:\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "        processor = BlipProcessor.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        \n",
    "        visualizer = CombinedTransformerVisualizer(model, processor, device)\n",
    "        results = visualizer.generate_visualizations(\n",
    "            image_path,\n",
    "            save_path='combined_visualization.png'\n",
    "        )\n",
    "        print(\"Visualization saved as 'combined_visualization.png'\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing image: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"./blip_radiology_finetuned_best\"  # Ensure the model path is correct\n",
    "    image_path = \"radiology_images/image_552.jpg\"  # Ensure the image path is correct\n",
    "    analyze_image(model_path, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paava\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.nn import functional as F\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTransformerVisualizer:\n",
    "    def __init__(self, model, processor, device='cuda'):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Target the output of the last transformer block\n",
    "        self.target_layer = self.model.vision_model.encoder.layers[-1]\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output[0]\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        # Register hooks for GradCAM\n",
    "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.backward_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "        \n",
    "        # Register hooks for attention maps on all encoder layers\n",
    "        self.attention_hooks = []\n",
    "        for layer in self.model.vision_model.encoder.layers:\n",
    "            hook = layer.self_attn.register_forward_hook(\n",
    "                lambda module, input, output: self._attention_hook(module, output)\n",
    "            )\n",
    "            self.attention_hooks.append(hook)\n",
    "\n",
    "    def _attention_hook(self, module, output):\n",
    "        \"\"\"Hook to capture attention maps during forward pass.\"\"\"\n",
    "        if isinstance(output, tuple) and len(output) > 1:\n",
    "            attention_weights = output[1]  # Usually, the attention weights are in the second element\n",
    "            \n",
    "            if attention_weights is not None:\n",
    "                print(\"Captured attention weights:\", attention_weights.shape)  # Debug print for attention weights\n",
    "                self.attention_maps.append(attention_weights.detach())  # Save the attention weights\n",
    "            else:\n",
    "                print(\"Warning: Attention weights are None.\")  # If no weights, print a warning\n",
    "        else:\n",
    "            print(\"Unexpected output format from attention layer:\", output)\n",
    "\n",
    "    def _perform_attention_rollout(self):\n",
    "        \"\"\"Perform attention rollout across all layers.\"\"\"\n",
    "        if not self.attention_maps:\n",
    "            raise ValueError(\"No attention maps captured. Check if the forward pass was successful.\")\n",
    "            \n",
    "        # Average attention heads per layer\n",
    "        averaged_attentions = [attn.mean(dim=1) for attn in self.attention_maps]\n",
    "        \n",
    "        # Start with identity matrix\n",
    "        batch_size, seq_len, _ = averaged_attentions[0].shape\n",
    "        accumulated = torch.eye(seq_len).unsqueeze(0).to(self.device)\n",
    "        accumulated = accumulated.repeat(batch_size, 1, 1)\n",
    "        \n",
    "        # Accumulate attention through layers\n",
    "        for attn in averaged_attentions:\n",
    "            accumulated = torch.bmm(attn, accumulated)\n",
    "        \n",
    "        # Get attention for tokens (exclude CLS token)\n",
    "        rollout = accumulated[:, 0, 1:]\n",
    "        \n",
    "        return rollout\n",
    "\n",
    "    def _process_attention_map(self, attention_map, image_size):\n",
    "        \"\"\"Process attention map for visualization.\"\"\"\n",
    "        attn = attention_map.cpu().numpy()\n",
    "        grid_size = int(np.sqrt(attn.shape[-1]))\n",
    "        attn = attn.reshape(grid_size, grid_size)\n",
    "        attn = cv2.resize(attn, (image_size[0], image_size[1]))\n",
    "        attn = gaussian_filter(attn, sigma=2)\n",
    "        attn = (attn - attn.min()) / (attn.max() - attn.min() + 1e-8)\n",
    "        return attn\n",
    "\n",
    "    def apply_threshold(self, cam, threshold=0.2):\n",
    "        \"\"\"Apply threshold to focus on high attention regions.\"\"\"\n",
    "        cam[cam < threshold] = 0\n",
    "        return cam\n",
    "\n",
    "    def generate_visualizations(self, image_path, save_path=None):\n",
    "        \"\"\"Generate both GradCAM and attention visualizations.\"\"\"\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        original_size = image.size\n",
    "        image_resized = image.resize((384, 384), Image.Resampling.LANCZOS)\n",
    "        inputs = self.processor(images=image_resized, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Clear previous attention maps\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Forward pass for attention maps\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.vision_model(\n",
    "                inputs['pixel_values'],\n",
    "                output_attentions=True,  # Ensure this is set to True\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        # Get attention rollout\n",
    "        rollout = self._perform_attention_rollout()\n",
    "        attention_map = self._process_attention_map(rollout[0], original_size)\n",
    "        \n",
    "        # Clear gradients and perform forward/backward pass for GradCAM\n",
    "        self.model.zero_grad()\n",
    "        outputs = self.model.vision_model(**inputs)\n",
    "        target = outputs.last_hidden_state.mean(dim=1).sum()\n",
    "        target.backward()\n",
    "        \n",
    "        if self.gradients is None or self.activations is None:\n",
    "            print(\"Error: Gradients or activations are None.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate GradCAM\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=1)\n",
    "        cam = torch.zeros(self.activations.shape[1], dtype=self.activations.dtype).to(self.device)\n",
    "        \n",
    "        for i in range(1, self.activations.shape[1]):\n",
    "            cam[i] = torch.sum(pooled_gradients[0] * self.activations[0, i])\n",
    "        \n",
    "        # Process GradCAM\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.detach().cpu().numpy()\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
    "        cam = self.apply_threshold(cam)\n",
    "        \n",
    "        # Reshape and resize GradCAM\n",
    "        grid_size = int(np.sqrt(len(cam) - 1))\n",
    "        cam_reshaped = cam[1:].reshape(grid_size, grid_size)\n",
    "        cam_resized = cv2.resize(cam_reshaped, original_size)\n",
    "        \n",
    "        # Create attention overlay\n",
    "        image_array = np.array(image)\n",
    "        attention_heatmap = cv2.applyColorMap(np.uint8(attention_map * 255), cv2.COLORMAP_JET)\n",
    "        attention_overlay = cv2.addWeighted(image_array, 0.7, attention_heatmap, 0.3, 0)\n",
    "        \n",
    "        # Create GradCAM overlay\n",
    "        gradcam_heatmap = cv2.applyColorMap(np.uint8(cam_resized * 255), cv2.COLORMAP_JET)\n",
    "        gradcam_overlay = cv2.addWeighted(image_array, 0.7, gradcam_heatmap, 0.3, 0)\n",
    "        \n",
    "        # Create GradCAM on Attention Overlay\n",
    "        gradcam_on_attention = cv2.addWeighted(attention_overlay, 0.5, gradcam_overlay, 0.5, 0)\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(25, 5))\n",
    "        \n",
    "        plt.subplot(1, 6, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 2)\n",
    "        plt.imshow(cam_resized, cmap='jet')\n",
    "        plt.title('GradCAM Heatmap')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 3)\n",
    "        plt.imshow(attention_map, cmap='jet')\n",
    "        plt.title('Attention Map')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 4)\n",
    "        plt.imshow(cv2.cvtColor(gradcam_overlay, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('GradCAM Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 5)\n",
    "        plt.imshow(cv2.cvtColor(attention_overlay, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Attention Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 6, 6)\n",
    "        plt.imshow(cv2.cvtColor(gradcam_on_attention, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('GradCAM on Attention Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        \n",
    "        return {\n",
    "            'gradcam': cam_resized,\n",
    "            'attention_map': attention_map,\n",
    "            'gradcam_overlay': gradcam_overlay,\n",
    "            'attention_overlay': attention_overlay,\n",
    "            'gradcam_on_attention': gradcam_on_attention\n",
    "        }\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up hooks.\"\"\"\n",
    "        self.forward_handle.remove()\n",
    "        self.backward_handle.remove()\n",
    "        for hook in self.attention_hooks:\n",
    "            hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(model_path, image_path, device='cuda'):\n",
    "    try:\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "        processor = BlipProcessor.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        \n",
    "        visualizer = CombinedTransformerVisualizer(model, processor, device)\n",
    "        results = visualizer.generate_visualizations(\n",
    "            image_path,\n",
    "            save_path='combined_visualization.png'\n",
    "        )\n",
    "        print(\"Visualization saved as 'combined_visualization.png'\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing image: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Captured attention weights: torch.Size([1, 16, 577, 577])\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Warning: Attention weights are None.\n",
      "Visualization saved as 'combined_visualization.png'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = \"../train/models/blip_radiology_finetuned\"  \n",
    "    image_path = \"../train/radiology_images/image_1294.jpg\"  \n",
    "    analyze_image(model_path, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
